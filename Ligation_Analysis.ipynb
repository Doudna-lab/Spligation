{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "159137ff",
   "metadata": {},
   "source": [
    "## SUMMARY\n",
    "\n",
    "Analysis of ligation products for spligation experiments.\n",
    "\n",
    "Briefly, for RT-PCR Nanopore data, each productâ€™s composition was confirmed by aligning reads to the corresponding amplicon with minimap2 (v2.28) and a custom junction file representing Csm cut sites. The abundance of each excision or ligation product was determined with HTSlib (v1.6)20, samtools (v1.6)21, and pysam (v0.18.0)22 and reported relative to total read depth within the crRNA target site.\n",
    "\n",
    "RNA-seq data (GEO accession number GSE220741) were analyzed as previously described in Colognori et al. 2023. Ligation products are assessed below using the bam files published in the original paper (Colognori et al. 2023).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0a2c4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### IMPORT LIBS ####\n",
    "import pandas as pd\n",
    "from pandas.errors import EmptyDataError \n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import subprocess\n",
    "import re\n",
    "import glob\n",
    "import pysamstats\n",
    "import pysam\n",
    "import Bio\n",
    "from Bio.Seq import reverse_complement\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "## SUPRESS WARNINGS\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## SLURM\n",
    "bash_header = [\"#!/bin/bash\", \"#SBATCH -p standard\", \"#SBATCH --job-name ALIGN_SPLIG\", \n",
    "               \"#SBATCH -o %j.out\", \"#SBATCH -e %j.err\", \"## ACTIVATE CONDA\", \n",
    "               'eval \"$(conda shell.bash hook)\"', 'conda activate Test_Py_3_9'\n",
    "              ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95e33be0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## PROCESSING LBR: TACATCTACTAATGCTCTTCTGGCTTT\n",
      "TAC------TAATGCTCTTCTGGCTTT\n",
      "TAC------------TCTTCTGGCTTT\n",
      "TAC------------------GGCTTT\n",
      "TAC------------------------\n",
      "TACATCTAC------TCTTCTGGCTTT\n",
      "TACATCTAC------------GGCTTT\n",
      "TACATCTAC------------------\n",
      "TACATCTACTAATGC------GGCTTT\n",
      "TACATCTACTAATGC------------\n",
      "TACATCTACTAATGCTCTTCT------\n",
      "## PROCESSING NCL: AAGTTTGAATAGCTTCTGTCCCTCTGC\n",
      "AAG------TAGCTTCTGTCCCTCTGC\n",
      "AAG------------CTGTCCCTCTGC\n",
      "AAG------------------CTCTGC\n",
      "AAG------------------------\n",
      "AAGTTTGAA------CTGTCCCTCTGC\n",
      "AAGTTTGAA------------CTCTGC\n",
      "AAGTTTGAA------------------\n",
      "AAGTTTGAATAGCTT------CTCTGC\n",
      "AAGTTTGAATAGCTT------------\n",
      "AAGTTTGAATAGCTTCTGTCC------\n",
      "## PROCESSING NPM1: AAGTCTCTTTAAGAAAATAGTTTAAAC\n",
      "AAG------TAAGAAAATAGTTTAAAC\n",
      "AAG------------AATAGTTTAAAC\n",
      "AAG------------------TTAAAC\n",
      "AAG------------------------\n",
      "AAGTCTCTT------AATAGTTTAAAC\n",
      "AAGTCTCTT------------TTAAAC\n",
      "AAGTCTCTT------------------\n",
      "AAGTCTCTTTAAGAA------TTAAAC\n",
      "AAGTCTCTTTAAGAA------------\n",
      "AAGTCTCTTTAAGAAAATAGT------\n",
      "GCG------GTTCTGTAGCAGTGTTCAAGTGG\n",
      "GCG------------TAGCAGTGTTCAAGTGG\n",
      "GCG------------------TGTTCAAGTGG\n",
      "GCG------------------------AGTGG\n",
      "GCGGATCAA------TAGCAGTGTTCAAGTGG\n",
      "GCGGATCAA------------TGTTCAAGTGG\n",
      "GCGGATCAA------------------AGTGG\n",
      "GCGGATCAAGTTCTG------TGTTCAAGTGG\n",
      "GCGGATCAAGTTCTG------------AGTGG\n",
      "GCGGATCAAGTTCTGTAGCAG------AGTGG\n",
      "GCGGATCAAGTTCTGTAGCAGTGTTCAAGTGG\n",
      "GCG-----------------------------|---GATCAAGTTCTGTCGCAGTGTTCAAGTGG\n",
      "GCG-----------------------------|---------GTTCTGTCGCAGTGTTCAAGTGG\n",
      "GCG-----------------------------|---------------TCGCAGTGTTCAAGTGG\n",
      "GCG-----------------------------|---------------------TGTTCAAGTGG\n",
      "GCG-----------------------------|---------------------------AGTGG\n",
      "GCGGATCAA-----------------------|---GATCAAGTTCTGTCGCAGTGTTCAAGTGG\n",
      "GCGGATCAA-----------------------|---------GTTCTGTCGCAGTGTTCAAGTGG\n",
      "GCGGATCAA-----------------------|---------------TCGCAGTGTTCAAGTGG\n",
      "GCGGATCAA-----------------------|---------------------TGTTCAAGTGG\n",
      "GCGGATCAA-----------------------|---------------------------AGTGG\n",
      "GCGGATCAAGTTCTG-----------------|---GATCAAGTTCTGTCGCAGTGTTCAAGTGG\n",
      "GCGGATCAAGTTCTG-----------------|---------GTTCTGTCGCAGTGTTCAAGTGG\n",
      "GCGGATCAAGTTCTG-----------------|---------------TCGCAGTGTTCAAGTGG\n",
      "GCGGATCAAGTTCTG-----------------|---------------------TGTTCAAGTGG\n",
      "GCGGATCAAGTTCTG-----------------|---------------------------AGTGG\n",
      "GCGGATCAAGTTCTGTCGCAG-----------|---GATCAAGTTCTGTCGCAGTGTTCAAGTGG\n",
      "GCGGATCAAGTTCTGTCGCAG-----------|---------GTTCTGTCGCAGTGTTCAAGTGG\n",
      "GCGGATCAAGTTCTGTCGCAG-----------|---------------TCGCAGTGTTCAAGTGG\n",
      "GCGGATCAAGTTCTGTCGCAG-----------|---------------------TGTTCAAGTGG\n",
      "GCGGATCAAGTTCTGTCGCAG-----------|---------------------------AGTGG\n",
      "GCGGATCAAGTTCTGTCGCAGTGTTCA-----|---GATCAAGTTCTGTCGCAGTGTTCAAGTGG\n",
      "GCGGATCAAGTTCTGTCGCAGTGTTCA-----|---------GTTCTGTCGCAGTGTTCAAGTGG\n",
      "GCGGATCAAGTTCTGTCGCAGTGTTCA-----|---------------TCGCAGTGTTCAAGTGG\n",
      "GCGGATCAAGTTCTGTCGCAGTGTTCA-----|---------------------TGTTCAAGTGG\n",
      "GCGGATCAAGTTCTGTCGCAGTGTTCA-----|---------------------------AGTGG\n",
      "##XIST_crNA_1\n",
      "# TARGET SITE:  CGGATCCAGTTCTGTCGCAGTGTTCAAGTGGC\n",
      "##XIST_crNA_2\n",
      "# TARGET SITE:  CATGGCGGGCTGTGCTTTGTTAGGTTGTCCAA\n"
     ]
    }
   ],
   "source": [
    "#### SETUP ####\n",
    "'''\n",
    "Configure strings corresponding to the crRNA target site.\n",
    "Strings for ligation/deletion products will be used for display\n",
    "'''\n",
    "\n",
    "pwd = os.getcwd()\n",
    "analysis_key = \"Spligation_Key_12162025.xlsx\"\n",
    "sup_table_df = pd.read_excel(\"Spligation_Key_12162025.xlsx\", sheet_name=\"Sup_Table_ROIs\")\n",
    "\n",
    "cut_indices = [3, 9, 15, 21] # Cut site locations\n",
    "deltas = [24, 18, 12, 6] # Deletion sizes\n",
    "\n",
    "# Target site strings\n",
    "target_dict = {\"LBR\":[\"TACATCTACTAATGCTCTTCTGGCTTT\", 27], # [String, length]\n",
    "               \"NCL\":[\"AAGTTTGAATAGCTTCTGTCCCTCTGC\", 27],\n",
    "               \"NPM1\":[\"AAGTCTCTTTAAGAAAATAGTTTAAAC\", 27]\n",
    "}\n",
    "\n",
    "# REF ROI COORDs\n",
    "ref_roi_coords = pd.read_excel(\"Spligation_Key_12162025.xlsx\", \n",
    "                               sheet_name = \"Ref_ROI_Locations\"\n",
    "                              )\n",
    "ligation_str_dict = {}\n",
    "\n",
    "# ENDOGENOUS\n",
    "for row in ref_roi_coords.itertuples():\n",
    "    ref = row.Construct\n",
    "    transcript = ref.split(\"_\")[0]\n",
    "    target_region = target_dict[transcript][0]\n",
    "    print(f\"## PROCESSING {transcript}: {target_region}\")\n",
    "    \n",
    "    # LIGATION STRINGS\n",
    "    cp_ends = [\"\".join(target_region[:n]) for n in cut_indices]\n",
    "    oh_ends = [\"\".join(target_region[n:]) for n in cut_indices]+['']\n",
    "    ligation_strs = [cp_end+(27-len(cp_end)-len(oh_end))*\"-\"+oh_end for cp_end in cp_ends for oh_end in oh_ends]\n",
    "    ligation_strs = [x for x in ligation_strs if len(x)==27 and \"-\" in x]\n",
    "    print('\\n'.join(ligation_strs))\n",
    "    \n",
    "    ligation_str_dict[transcript] = ligation_strs\n",
    "    \n",
    "# 1 CUT Cis: 'Cis_1Cut'\n",
    "reporter_target_region = 'GCGGATCAAGTTCTGTAGCAGTGTTCAAGTGG'\n",
    "cut_indices = [3, 9, 15, 21, 27] # Plot full target region (32nt)\n",
    "cp_ends = [\"\".join(reporter_target_region[:n]) for n in cut_indices]\n",
    "oh_ends = [\"\".join(reporter_target_region[n:]) for n in cut_indices]\n",
    "ligation_strs = [cp_end+(32-len(cp_end)-len(oh_end))*\"-\"+oh_end for cp_end in cp_ends for oh_end in oh_ends]\n",
    "ligation_strs = [x for x in ligation_strs if len(x)==32 and \"-\" in x]+[reporter_target_region]\n",
    "print('\\n'.join(ligation_strs))\n",
    "ligation_str_dict['Cis_1Cut'] = ligation_strs\n",
    "\n",
    "# 2 CUT Cis: '2Cut_Short_Cis_Amplicon', '2Cut_Long_Cis_Amplicon'\n",
    "large_del_conversion_dict = {}\n",
    "n_term = \"GCGGATCAAGTTCTGTCGCAGTGTTCAAGTGG\"\n",
    "cp_ends = [\"\".join(n_term[:n]) for n in cut_indices]\n",
    "oh_ends = [\"\".join(n_term[n:]) for n in cut_indices]\n",
    "formatted_large_del_ligation_strs = [cp_end+(27-len(cp_end))*\"-\"+\"-----|\"\\\n",
    "                 +\"---\"\\\n",
    "                 +(29-len(oh_end))*\"-\"+oh_end for cp_end in cp_ends for oh_end in oh_ends]\n",
    "\n",
    "print('\\n'.join(formatted_large_del_ligation_strs))\n",
    "large_del_conditions = ['Cis_2CutShort', 'Cis_2CutLong']\n",
    "\n",
    "for large_del_condition in large_del_conditions:\n",
    "    del_row = sup_table_df.loc[sup_table_df.Analysis == large_del_condition]\n",
    "    del_start = int(del_row.ROI_Skip_Start.values[0])\n",
    "    del_stop = int(del_row.ROI_Skip_Stop.values[0])\n",
    "    del_str = (del_stop-del_start)*\"-\"\n",
    "    large_del_ligation_strs = [cp_end+(27-len(cp_end))*\"-\"+\"|\"\\\n",
    "                 +(29-len(oh_end))*\"-\"+oh_end for cp_end in cp_ends for oh_end in oh_ends]\n",
    "    temp_lig_strs = [x.replace('|', f'{del_str}') for x in large_del_ligation_strs]\n",
    "    ligation_str_dict[large_del_condition] = temp_lig_strs\n",
    "    large_del_conversion_dict[large_del_condition] = dict(zip(temp_lig_strs,formatted_large_del_ligation_strs))\n",
    "\n",
    "# TRANS\n",
    "n_term = \"GCGGATCAAGTTCTGTCGCAGTGTTCAAGTGG\"\n",
    "cp_ends = [\"\".join(n_term[:n]) for n in cut_indices]\n",
    "oh_ends = [\"\".join(n_term[n:]) for n in cut_indices]\n",
    "trans_del_ligation_strs = [cp_end+(27-len(cp_end))*\"-\"+\n",
    "                 +(29-len(oh_end))*\"-\"+oh_end for cp_end in cp_ends for oh_end in oh_ends]\n",
    "clean_pipe = [x.replace('|', f'') for x in trans_del_ligation_strs]\n",
    "ligation_str_dict['Trans_Basic'] = clean_pipe\n",
    "trans_conversion_dict = dict(zip(clean_pipe,formatted_large_del_ligation_strs))\n",
    "\n",
    "# XIST\n",
    "xist_conditions = ['XIST_crNA_1', 'XIST_crNA_2']\n",
    "xist_conversion_dict = {}\n",
    "for xist_condition in xist_conditions:\n",
    "    print(f\"##{xist_condition}\")\n",
    "    xist_row = sup_table_df.loc[sup_table_df.Analysis == xist_condition]\n",
    "    crrna_str = xist_row.ROI_Str.values[0]\n",
    "    target_site = str(Bio.Seq.Seq(crrna_str).reverse_complement())\n",
    "    \n",
    "    print(\"# TARGET SITE: \",target_site)\n",
    "    cut_indices = [3, 9, 15, 21]\n",
    "    deltas = [24, 18, 12, 6]\n",
    "\n",
    "    cut_indices = [3, 9, 15, 21, 27] # Plot full target region (32nt)\n",
    "    cp_ends = [\"\".join(target_site[:n]) for n in cut_indices]\n",
    "    oh_ends = [\"\".join(target_site[n:]) for n in cut_indices]\n",
    "    ligation_strs = [cp_end+(32-len(cp_end)-len(oh_end))*\"-\"+oh_end for cp_end in cp_ends for oh_end in oh_ends]\n",
    "    ligation_strs = [x for x in ligation_strs if len(x)==32 and \"-\" in x]+[target_site]\n",
    "    ligation_str_dict[xist_condition] = ligation_strs\n",
    "    \n",
    "    conversion_strs = [str(Bio.Seq.Seq(x).reverse_complement()) for x in ligation_strs]\n",
    "    xist_conversion_dict[xist_condition] = dict(zip(conversion_strs,ligation_strs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a757205",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBR_Amplicon_With_Target_Site.fasta TACATCTACTAATGCTCTTCTGGCTTT ['TAC', 'TACATCTAC', 'TACATCTACTAATGC', 'TACATCTACTAATGCTCTTCT', 'TACATCTACTAATGCTCTTCTGGCTTT']\n",
      "NCL_Amplicon_With_Target_Site.fasta AAGTTTGAATAGCTTCTGTCCCTCTGC ['AAG', 'AAGTTTGAA', 'AAGTTTGAATAGCTT', 'AAGTTTGAATAGCTTCTGTCC', 'AAGTTTGAATAGCTTCTGTCCCTCTGC']\n",
      "NPM1_Amplicon_With_Target_Site.fasta AAGTCTCTTTAAGAAAATAGTTTAAAC ['AAG', 'AAGTCTCTT', 'AAGTCTCTTTAAGAA', 'AAGTCTCTTTAAGAAAATAGT', 'AAGTCTCTTTAAGAAAATAGTTTAAAC']\n"
     ]
    }
   ],
   "source": [
    "#### PROCESS REFERENCES ####\n",
    "'''\n",
    "Create splice junction file corresponding to 6nt cut sites characteristic of CRISPR-Csm\n",
    "'''\n",
    "\n",
    "contig_junction_dict = {}\n",
    "start_stop_str_dicts = {}\n",
    "\n",
    "for idx,roi in ref_roi_coords.iterrows():\n",
    "    construct = roi['Construct']\n",
    "    target_starts = [int(x) for x in roi[['Target1_0Based_Start', 'Target2_0Based_Start']].values if str(x) != \"nan\"]\n",
    "\n",
    "    seq = target_dict[construct.split('_')[0]][0]\n",
    "    max_str_len = target_dict[construct.split('_')[0]][-1]\n",
    "\n",
    "    \n",
    "    # LIGATION STRINGS\n",
    "    cp_ends = [\"\".join(seq[:n]) for n in cut_indices]\n",
    "    oh_ends = [\"\".join(seq[n:]) for n in cut_indices]+['']\n",
    "    len_cp_ends = [len(cp_end) for cp_end in cp_ends]\n",
    "    len_oh_ends = [len(oh_end) for oh_end in oh_ends]\n",
    "    ligation_strs = [cp_end+(27-len(cp_end)-len(oh_end))*\"-\"+oh_end for cp_end in cp_ends for oh_end in oh_ends]\n",
    "    ligation_strs = [x for x in ligation_strs if len(x)==27 and \"-\" in x]\n",
    "    print(construct,seq, cp_ends)\n",
    "    \n",
    "    # DEFINE JUNCTION SITES\n",
    "    ligation_locs = [f'{len(cp_end)+target_starts[0]}__{len(cp_end)+target_starts[0]+(max_str_len-len(cp_end)-len(oh_end))}' for cp_end in cp_ends for oh_end in oh_ends]\n",
    "    start_stop_str_dict = dict(zip(ligation_locs, ligation_strs))\n",
    "    start_stop_str_dicts[construct] = start_stop_str_dict\n",
    "    #print(len_cp_ends, len_oh_ends, f\"!!!!{len(ligation_strs)}\")\n",
    "    #print(\"\\n\".join([f\"{ligation_str}: {len(ligation_str)}, {ligation_locs[idx]}\" for idx,ligation_str in enumerate(ligation_strs)]))\n",
    "\n",
    "    # SAVE BED6\n",
    "    bed6_lines = []\n",
    "    strand = \"+\"\n",
    "    temp_dict = {}\n",
    "    \n",
    "    for i, loc in enumerate(ligation_locs):\n",
    "        start, end = loc.split(\"__\")\n",
    "        bed6_lines.append(f\"{construct.replace('.fasta','')}\\t{start}\\t{end}\\tjunc{i+1}\\t0\\t{strand}\")\n",
    "        temp_dict[f\"junc{i+1}\"] = loc\n",
    "    contig_junction_dict[construct]= temp_dict\n",
    "\n",
    "    # Save to file\n",
    "    with open(f\"References/{construct.replace('.fasta','')}_junctions.bed\", \"w\") as f:\n",
    "        for line in bed6_lines:\n",
    "            f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b604fddb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## PROCESSING: LBR__D9S6PX_1_EndoDeluxe-LBR\n",
      "/groups/doudna/projects/mtrinidad_projects/Csm_RNASeq_David/Spligation/Nanopore_12182025/Data/D9S6PX_1_EndoDeluxe-LBR.fastq\n",
      "/groups/doudna/projects/mtrinidad_projects/Csm_RNASeq_David/Spligation/Nanopore_12182025/References/LBR_Amplicon_With_Target_Site.fasta\n",
      "## PROCESSING: NCL__D9S6PX_2_EndoDeluxe-NCL\n",
      "/groups/doudna/projects/mtrinidad_projects/Csm_RNASeq_David/Spligation/Nanopore_12182025/Data/D9S6PX_2_EndoDeluxe-NCL.fastq\n",
      "/groups/doudna/projects/mtrinidad_projects/Csm_RNASeq_David/Spligation/Nanopore_12182025/References/NCL_Amplicon_With_Target_Site.fasta\n",
      "## PROCESSING: NPM1__D9S6PX_3_EndoDeluxe-NPM1\n",
      "/groups/doudna/projects/mtrinidad_projects/Csm_RNASeq_David/Spligation/Nanopore_12182025/Data/D9S6PX_3_EndoDeluxe-NPM1.fastq\n",
      "/groups/doudna/projects/mtrinidad_projects/Csm_RNASeq_David/Spligation/Nanopore_12182025/References/NPM1_Amplicon_With_Target_Site.fasta\n"
     ]
    }
   ],
   "source": [
    "#### SETUP ALIGNMENTS ####\n",
    "'''\n",
    "Align Nanopore data to amplicon from RT-PCR experiments\n",
    "'''\n",
    "\n",
    "threads = \"$SLURM_CPUS_ON_NODE\"\n",
    "bash_lines = bash_header\n",
    "bash_script_name = \"Nanopore_Alignments.sh\"\n",
    "\n",
    "alignment_key = pd.read_excel(\"Spligation_Key_12162025.xlsx\", sheet_name=\"Alignment_Key\")\n",
    "\n",
    "\n",
    "data_dir = f\"{pwd}/Data/\"\n",
    "ref_dir = f\"{pwd}/References/\"\n",
    "alignment_dir = f\"{pwd}/Alignments/\"\n",
    "\n",
    "fastas = glob.glob(f\"References/*.fasta\")\n",
    "juncs = glob.glob(f\"References/*junctions.bed\")\n",
    "fastqs = glob.glob(f\"{pwd}/Data/*.fastq\")\n",
    "\n",
    "qcd = []\n",
    "bams_1ref_basic = {}\n",
    "bams_1ref_splice = {}\n",
    "for idx,row in alignment_key.iterrows(): \n",
    "    analysis_id = row['Sample_Description'].replace(' ', \"_\")\n",
    "    fq = f'{data_dir}{row[\"FASTQ_Name\"]}'\n",
    "    sample_id = f\"{analysis_id}__\" + fq.split(\"/\")[-1].split(\".\")[0]\n",
    "    ref_1seq = f\"{ref_dir}{row.Reference_for_Alignment}.fasta\"\n",
    "    \n",
    "    print(f\"## PROCESSING: {sample_id}\")\n",
    "    print(fq)\n",
    "    print(ref_1seq)\n",
    "    \n",
    "\n",
    "    bash_lines.append(f'#### {analysis_id}')\n",
    "    \n",
    "    # QC\n",
    "    qc_dir = data_dir\n",
    "    if fq not in qcd:\n",
    "        bash_lines.append(\"# QC\")\n",
    "        qc_str = f\"NanoPlot -t {threads} --fastq {fq} --maxlength 10000 --plots dot --prefix {sample_id} --outdir {qc_dir}\"\n",
    "        bash_lines.append(qc_str)\n",
    "        qcd.append(fq)\n",
    "        \n",
    "    \n",
    "    # 1REF SPLICE MODE ALIGNMENT\n",
    "    bash_lines.append(\"# 1REF SPLICE MODE ALIGNMENT\")\n",
    "    bam_1ref_splice = f\"{alignment_dir}{sample_id}__1ref_splice_sorted.bam\"\n",
    "    bams_1ref_splice[sample_id] = bam_1ref_splice\n",
    "    align_1ref_splice = (f\"minimap2 -ax splice -k12 -t {threads} -J 1 {ref_1seq} {fq} \"\n",
    "                         f\"| samtools view -bS -@ {threads} - \"\n",
    "                         f\"| samtools sort -@ {threads} -o {bam_1ref_splice} - && \"\n",
    "                         f\"samtools index  -@ {threads} {bam_1ref_splice}\"\n",
    "                        )\n",
    "    bash_lines.append(align_1ref_splice)\n",
    "\n",
    "with open(bash_script_name, \"w\") as f:\n",
    "    f.writelines(\"\\n\".join(bash_lines))\n",
    "\n",
    "\n",
    "#!sbatch Nanopore_Alignments.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d1a1bb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBR__D9S6PX_1_EndoDeluxe-LBR\n",
      "NCL__D9S6PX_2_EndoDeluxe-NCL\n",
      "NPM1__D9S6PX_3_EndoDeluxe-NPM1\n"
     ]
    }
   ],
   "source": [
    "#### SETUP BIGWIG COVERAGE TRACKS\n",
    "'''\n",
    "Create bigwigs for IGV coverage tracks\n",
    "'''\n",
    "threads = \"$SLURM_CPUS_ON_NODE\"\n",
    "bash_lines = bash_header\n",
    "bash_script_name = \"Bigwigs.sh\"\n",
    "\n",
    "for sample_id, bam in bams_1ref_splice.items():  \n",
    "    print(sample_id)\n",
    "    ## BIGWIG\n",
    "    bash_lines.append(f\"# BigWig\")\n",
    "    bw_str = f\"bamCoverage -b {bam} -o {bam.replace('.bam', '.bw')} --numberOfProcessors {threads} --binSize 1\"\n",
    "    bash_lines.append(bw_str)\n",
    "    \n",
    "    bash_lines.append(f\"# CPM BigWig\")\n",
    "    bw_cpm_str = f\"bamCoverage -b {bam} -o {bam.replace('.bam', '_CPM.bw')} --normalizeUsing CPM --numberOfProcessors {threads} --binSize 1\"\n",
    "    bash_lines.append(bw_cpm_str)\n",
    "    #print(bw_str, bw_cpm_str)\n",
    "\n",
    "with open(bash_script_name, \"w\") as f:\n",
    "    f.writelines(\"\\n\".join(bash_lines))\n",
    "\n",
    "#!sbatch Bigwigs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d104233b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Processing LBR\n",
      "#### Processing NCL\n",
      "#### Processing NPM1\n"
     ]
    }
   ],
   "source": [
    "#### TABULATE ENDOGENOUS LIGATION: sfGFP #####\n",
    "'''\n",
    "Tabulate ligation products for endogenous spligation of C-term sfGFP\n",
    "'''\n",
    "\n",
    "final_count_dict = {}\n",
    "full_span_dict = {}\n",
    "for alignment_id, bam_file in bams_1ref_splice.items():\n",
    "    ref_id = alignment_id.split(\"__\")[0]\n",
    "    ref = alignment_key.loc[\n",
    "        alignment_key.Sample_Description == ref_id\n",
    "    ].Reference_for_Alignment.values[0]\n",
    "\n",
    "    print(f\"#### Processing {ref_id}\")\n",
    "    bam = pysam.AlignmentFile(bam_file, \"rb\")\n",
    "\n",
    "    start = ref_roi_coords[\n",
    "        ref_roi_coords.Construct.str.startswith(ref)\n",
    "    ].Target1_0Based_Start.values[0]\n",
    "    end = start + 27\n",
    "\n",
    "    counts = Counter()\n",
    "\n",
    "    for read in bam.fetch(ref, start, end):\n",
    "        if read.is_unmapped:\n",
    "            continue\n",
    "\n",
    "        # FULL-SPAN REQUIREMENT\n",
    "        if read.reference_start > start or read.reference_end < end:\n",
    "            continue\n",
    "\n",
    "        aligned = []\n",
    "        ref_pos = read.reference_start\n",
    "        query_pos = 0\n",
    "\n",
    "        for op, length in read.cigartuples:\n",
    "\n",
    "            # M, =, X: matches\n",
    "            if op in (0, 7, 8):\n",
    "                for _ in range(length):\n",
    "                    if start <= ref_pos < end:\n",
    "                        aligned.append(read.query_sequence[query_pos])\n",
    "                    ref_pos += 1\n",
    "                    query_pos += 1\n",
    "\n",
    "            # I: insertions\n",
    "            elif op == 1:\n",
    "                if start <= ref_pos < end:\n",
    "                    aligned.append(read.query_sequence[query_pos:query_pos + length])\n",
    "                query_pos += length\n",
    "\n",
    "            # D: deletions\n",
    "            elif op in (2,3):\n",
    "                for _ in range(length):\n",
    "                    if start <= ref_pos < end:\n",
    "                        aligned.append(\"-\")\n",
    "                    ref_pos += 1\n",
    "\n",
    "            # Soft clip\n",
    "            elif op == 4:\n",
    "                query_pos += length\n",
    "\n",
    "            # Hard clip\n",
    "            elif op == 5:\n",
    "                pass\n",
    "\n",
    "        seq = \"\".join(aligned)\n",
    "\n",
    "        # Increment string count\n",
    "        if len(seq) > 0:\n",
    "            counts[seq] += 1\n",
    "\n",
    "    #for seq, count in counts.most_common():\n",
    "    #    print(count, seq)\n",
    "\n",
    "    final_count_dict[ref_id] = counts\n",
    "    \n",
    "    # COUNT TOTAL FULL-SPAN READS\n",
    "    full_span_count = 0\n",
    "\n",
    "    for read in bam.fetch(ref, start, end):\n",
    "        if read.is_unmapped:\n",
    "            continue\n",
    "        if read.reference_start <= start and read.reference_end >= end:\n",
    "            full_span_count += 1\n",
    "            \n",
    "    full_span_dict[alignment_id.split(\"_\")[0]] = full_span_count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bd30719",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################### TABULATE PRIOR LIGATIONS ######################################\n",
    "sup_table_df = pd.read_excel(\"Spligation_Key_12162025.xlsx\", sheet_name=\"Sup_Table_ROIs\")\n",
    "final_count_dict_prior = {}\n",
    "full_span_dict_prior = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de60c854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Processing XIST_crNA_1\n",
      "\tGCCACTTGAACACTGCGACAGAACTGGATCCG\n",
      "#### Processing XIST_crNA_2\n",
      "\tTTGGACAACCTAACAAAGCACAGCCCGCCATG\n",
      "#### Processing Cis_1Cut\n",
      "\tGCGGATCAAGTTCTGTAGCAGTGTTCAAGTGG\n",
      "#### Processing Trans_Basic\n",
      "\tGCGGATCAAGTTCTGTCGCAGTGTTCAGATCAAGTTCTGTCGCAGTGTTCAAGTGG\n"
     ]
    }
   ],
   "source": [
    "#### TABULATE Prior: normal amplicon Ref ####\n",
    "'''\n",
    "Tabulate ligation products for prior XIST and reporter experiments\n",
    "'''\n",
    "hg38_processing = ['XIST_crNA_1', 'XIST_crNA_2']\n",
    "normal_processing = ['Cis_1Cut', 'Trans_Basic'] + hg38_processing\n",
    "\n",
    "normal_processing_df = sup_table_df.loc[sup_table_df.Analysis.isin(normal_processing)]\n",
    "\n",
    "final_count_dict_normal_prior = {}\n",
    "\n",
    "for row in normal_processing_df.itertuples():\n",
    "    alignment_id = row.Analysis\n",
    "    bam_file = row.BAM\n",
    "    chromo = row.chr\n",
    "    roi_str = row.ROI_Str\n",
    "\n",
    "    print(f\"#### Processing {alignment_id}\")\n",
    "    print(f\"\\t{roi_str}\")\n",
    "    bam = pysam.AlignmentFile(bam_file, \"rb\")\n",
    "\n",
    "    start = row.ROI_Start\n",
    "    end = start + len(roi_str)\n",
    "\n",
    "    counts = Counter()\n",
    "\n",
    "    for read in bam.fetch(chromo, start, end):\n",
    "        # SKIP UNMAPPED READS\n",
    "        if read.is_unmapped:\n",
    "            continue\n",
    "\n",
    "        # FULL-SPAN REQUIREMENT\n",
    "        if read.reference_start > start or read.reference_end < end:\n",
    "            continue\n",
    "\n",
    "        aligned = []\n",
    "        ref_pos = read.reference_start\n",
    "        query_pos = 0\n",
    "\n",
    "\n",
    "        for op, length in read.cigartuples:\n",
    "            # M, =, X: matches\n",
    "            if op in (0, 7, 8):\n",
    "                for i in range(length):\n",
    "                    if start <= ref_pos < end:\n",
    "                        aligned.append(read.query_sequence[query_pos])\n",
    "                    ref_pos += 1\n",
    "                    query_pos += 1\n",
    "\n",
    "            # I: insertions\n",
    "            elif op == 1:\n",
    "                if start <= ref_pos < end:\n",
    "                    aligned.append(read.query_sequence[query_pos:query_pos+length])\n",
    "                query_pos += length\n",
    "\n",
    "            # D: deletions\n",
    "            elif op in (2, 3):\n",
    "                for i in range(length):\n",
    "                    if start <= ref_pos < end:\n",
    "                        aligned.append(\"-\")\n",
    "                    ref_pos += 1\n",
    "\n",
    "            # Soft clip\n",
    "            elif op == 4:\n",
    "                query_pos += length\n",
    "\n",
    "            # Hard clip (ignore)\n",
    "            elif op == 5:\n",
    "                pass\n",
    "\n",
    "        seq = \"\".join(aligned)\n",
    "\n",
    "        # Increment count\n",
    "        if len(seq) > 0:\n",
    "            counts[seq] += 1\n",
    "\n",
    "    final_count_dict_prior[alignment_id] = counts\n",
    "    \n",
    "    \n",
    "    # COUNT TOTAL FULL-SPAN READS\n",
    "    full_span_count = 0\n",
    "\n",
    "    for read in bam.fetch(chromo, start, end):\n",
    "        if read.is_unmapped:\n",
    "            continue\n",
    "        if read.reference_start <= start and read.reference_end >= end:\n",
    "            full_span_count += 1\n",
    "            \n",
    "    full_span_dict_prior[alignment_id] = full_span_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdc109bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Processing Cis_2CutShort\n",
      "\tGCGGATCAAGTTCTGTCGCAGTGTTCAGATCAAGTTCTGTCGCAGTGTTCAAGTGG\n",
      "#### Processing Cis_2CutLong\n",
      "\tGCGGATCAAGTTCTGTCGCAGTGTTCAGATCAAGTTCTGTCGCAGTGTTCAAGTGG\n"
     ]
    }
   ],
   "source": [
    "#### PROCESS PRIOR: Ref with deletion ####\n",
    "'''\n",
    "Tabulate ligation products for 2-cut reporters with longer ROIs\n",
    "(as defined by \"end\" variable in line 22)\n",
    "'''\n",
    "del_processing = ['Cis_2CutShort', 'Cis_2CutLong']\n",
    "\n",
    "del_processing_df = sup_table_df.loc[sup_table_df.Analysis.isin(del_processing)]\n",
    "\n",
    "final_count_dict_del_prior = {}\n",
    "\n",
    "for row in del_processing_df.itertuples():\n",
    "    alignment_id = row.Analysis\n",
    "    bam_file = row.BAM\n",
    "    chromo = row.chr\n",
    "    roi_str = row.ROI_Str\n",
    "\n",
    "    print(f\"#### Processing {alignment_id}\")\n",
    "    print(f\"\\t{roi_str}\")\n",
    "    bam = pysam.AlignmentFile(bam_file, \"rb\")\n",
    "\n",
    "    start = row.ROI_Start\n",
    "    end = row.ROI_Stop+1\n",
    "\n",
    "    counts = Counter()\n",
    "\n",
    "    for read in bam.fetch(chromo, start, end):\n",
    "        # SKIP UNMAPPED READS\n",
    "        if read.is_unmapped:\n",
    "            continue\n",
    "\n",
    "        # FULL-SPAN REQUIREMENT\n",
    "        if read.reference_start > start or read.reference_end < end:\n",
    "            continue\n",
    "\n",
    "        aligned = []\n",
    "        ref_pos = read.reference_start\n",
    "        query_pos = 0\n",
    "\n",
    "\n",
    "        for op, length in read.cigartuples:\n",
    "            # M, =, X : Matches\n",
    "            if op in (0, 7, 8):\n",
    "                for i in range(length):\n",
    "                    if start <= ref_pos < end:\n",
    "                        aligned.append(read.query_sequence[query_pos])\n",
    "                    ref_pos += 1\n",
    "                    query_pos += 1\n",
    "\n",
    "            # I: insertions\n",
    "            elif op == 1:\n",
    "                if start <= ref_pos < end:\n",
    "                    aligned.append(read.query_sequence[query_pos:query_pos+length])\n",
    "                query_pos += length\n",
    "\n",
    "            # D: deletions\n",
    "            elif op in (2, 3):\n",
    "                for i in range(length):\n",
    "                    if start <= ref_pos < end:\n",
    "                        aligned.append(\"-\")\n",
    "                    ref_pos += 1\n",
    "\n",
    "            # Soft clip\n",
    "            elif op == 4:\n",
    "                query_pos += length\n",
    "\n",
    "            # Hard clip (ignore)\n",
    "            elif op == 5:\n",
    "                pass\n",
    "\n",
    "        seq = \"\".join(aligned)\n",
    "\n",
    "\n",
    "        if len(seq) > 0:\n",
    "            counts[seq] += 1\n",
    "\n",
    "    final_count_dict_prior[alignment_id] = counts\n",
    "    \n",
    "    \n",
    "    # COUNT TOTAL FULL-SPAN READS\n",
    "    full_span_count = 0\n",
    "\n",
    "    for read in bam.fetch(chromo, start, end):\n",
    "        if read.is_unmapped:\n",
    "            continue\n",
    "        if read.reference_start <= start and read.reference_end >= end:\n",
    "            full_span_count += 1\n",
    "            \n",
    "    full_span_dict_prior[alignment_id] = full_span_count\n",
    "\n",
    "final_count_dict.update(final_count_dict_prior)\n",
    "full_span_dict.update(full_span_dict_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e24a4be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### PROCESSING: LBR\n",
      "#### PROCESSING: NCL\n",
      "#### PROCESSING: NPM1\n",
      "#### PROCESSING: XIST_crNA_1\n",
      "#### PROCESSING: XIST_crNA_2\n",
      "#### PROCESSING: Cis_1Cut\n",
      "#### PROCESSING: Trans_Basic\n",
      "#### PROCESSING: Cis_2CutShort\n",
      "#### PROCESSING: Cis_2CutLong\n"
     ]
    }
   ],
   "source": [
    "#### SAVE ####\n",
    "'''\n",
    "Configure counts with display strings and save to Excel file\n",
    "'''\n",
    "\n",
    "# Adjust strings for minor alignment artifacts\n",
    "fix_strs = {'LBR':{'TACATCTACTAATGCTCTTC------T':'TACATCTACTAATGCTCTTCT------'},\n",
    "            'NCL':{},\n",
    "            'NPM1':{},\n",
    "            'Cis_1Cut':{'GCGGATCA------------------AAGTGG':'GCGGATCAA------------------AGTGG'},\n",
    "            'Cis_2CutShort':{}, \n",
    "            'Cis_2CutLong':{},\n",
    "            'Trans_Basic':{},\n",
    "            'XIST_crNA_1':{},\n",
    "            'XIST_crNA_2':{}\n",
    "            \n",
    "            \n",
    "}\n",
    "\n",
    "## COLLECT DATA\n",
    "dfs = {}\n",
    "for sample_id, data in final_count_dict.items():\n",
    "    \n",
    "    print(f\"#### PROCESSING: {sample_id}\")\n",
    "    \n",
    "    # FIX\n",
    "    fix_dict = fix_strs[sample_id]\n",
    "    for bad_str,new_str in fix_dict.items():\n",
    "        new_value = data[bad_str] + data[new_str]\n",
    "        data[new_str] = new_value\n",
    "    \n",
    "    # FILTER DICT FOR KNOWN LIGATION PRODUCTS\n",
    "    if 'XIST' in sample_id: # If Xist, convert to standardize target-site str format\n",
    "        temp_converter = xist_conversion_dict[sample_id]\n",
    "        data_clean = {temp_converter[k]:v for k,v in data.items() if k in temp_converter.keys()}\n",
    "        data_clean.update({k:0 for k in ligation_str_dict[sample_id] if k not in data_clean.keys()})\n",
    "    elif '2Cut' in sample_id:\n",
    "        data.update({k:0 for k in ligation_str_dict[sample_id] if k not in data.keys()})\n",
    "        temp_converter = large_del_conversion_dict[sample_id]\n",
    "        data_clean = {temp_converter[k]:v for k,v in data.items() if k in temp_converter.keys()}\n",
    "        data_clean.update({k:0 for k in ligation_str_dict[sample_id] if k not in data_clean.keys()})\n",
    "    elif \"Trans\" in sample_id:\n",
    "        data.update({k:0 for k in ligation_str_dict[sample_id] if k not in data.keys()})\n",
    "        temp_converter = trans_conversion_dict\n",
    "        data_clean = {temp_converter[k]:v for k,v in data.items() if k in temp_converter.keys()}\n",
    "    else:\n",
    "        data.update({k:0 for k in ligation_str_dict[sample_id] if k not in data.keys()})\n",
    "        data_clean = {k:v for k,v in data.items() if k in ligation_str_dict[sample_id]}\n",
    "    \n",
    "    \n",
    "    n_spanning_reads = full_span_dict[sample_id]\n",
    "\n",
    "    data_clean['Other_Reads'] = n_spanning_reads - sum(data_clean.values())\n",
    "    data_clean['Total_Reads'] = n_spanning_reads\n",
    "    \n",
    "    # Create DF\n",
    "    df = pd.DataFrame({'Ligation_Product':data_clean.keys(),\n",
    "                       'N_Reads':data_clean.values()\n",
    "                      })\n",
    "    \n",
    "    df['Percent'] = 100*df['N_Reads']/n_spanning_reads\n",
    "    dfs[sample_id] = df\n",
    "\n",
    "    \n",
    "## SAVE\n",
    "with pd.ExcelWriter(\"Junction_Counts_12222025.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    for sample_id, df in dfs.items():\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        # Save to Excel sheet\n",
    "        sheet_name = sample_id[:31] # 31 chars max\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Test_Py_3_9",
   "language": "python",
   "name": "test_py_3_9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
